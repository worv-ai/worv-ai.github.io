<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="We propose a framework for context-aware robot navigation that excels in both simulated and real-world environments.">
    <meta name="keywords" content="Vision-Language-Action (VLA) Models, Imitation Learning, Multimodal Instruction Following">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5J9LZW868J"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5J9LZW868J');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="../static/css/bulma.min.css">
    <link rel="stylesheet" href="../static/css/slick.css">
    <link rel="stylesheet" href="../static/css/slick-theme.css">
    <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="../static/css/index.css">
    <link rel="icon" href="./static/images/favicon.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="../static/js/fontawesome.all.min.js"></script>
    <script src="../static/js/slick.min.js"></script>
    <script src="../static/js/index.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://worv-ai.github.io/">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://worv-ai.github.io/">
                            Not released yet
                        </a>
                    </div>
                </div>
            </div>

        </div>
    </nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">CANVAS: Commonsense-Aware Navigation System
                            for Intuitive Human-Robot Interaction</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Suhwan Choi<sup>†1</sup>,</span>
                            <span class="author-block">
                                Yongjun Cho<sup>†1</sup>,</span>
                            <span class="author-block">
                                Minchan Kim<sup>†1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Jaeyoon Jung<sup>†1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Myunchul Joe<sup>1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Yubeen Park<sup>1</sup>,</span>
                            </span>
                            <br>
                            <span class="author-block">
                                Minseo Kim<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Sungwoong Kim<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Sungjae Lee<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Hwiseong Park<sup>1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Jiwan Chung<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Youngjae Yu<sup>2</sup>,</span>
                            </span>
                        </div>

                        <div class="is-size-6 publication-authors">
                            <span class="author-block">
                                <sup>†</sup> Equal contribution, <sup>1</sup> MAUM.AI, <sup>2</sup> Yonsei University
                        </div>
                        <br>

                        <div class="is-size-5 publication-venue">
                          <span class="venue-block">Under Review</span> <br>
                        </div>
                        <br>

                        <div class="is-size-6 publication-venue">
                            <span class="venue-block"><span class="publication-awards">Oral Talk </span> at NeurIPS 2024 Workshop Open-World Agents</span> <br>
                          </div> 

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2410.01273"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/oJuU4x02azI"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <!-- <span class="link-block">
                                    <a href="https://github.com/worv-ai/canvas"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github-alt"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span> -->
                                <!-- Model Link. -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/maum-ai/CANVAS-S"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-robot"></i>
                                        </span>
                                        <span>Model</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/worv-ai/canvas"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i> </span>
                                        <span>Data</span>
                                    </a>
                                </span>
                                <!-- Demo Link. -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/spaces/maum-ai/CANVAS-DEMO"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-images"></i> </span>
                                        <span>Demo</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop is-centered has-text-justified is-size-5">
            <div class="hero-body">
                <figure id="teaser">
                    <img src="./static/images/1_teaser.png" alt="canvas teaser" />
                </figure>
                <p>
                    CANVAS is a novel framework for commonsense-aware robot navigation that excels in both simulated and real-world environments. 
                </p>
            </div>
        </div>
    </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-orchard">
                        <video poster="" id="orchard" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/orchard_canvas.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-street">
                        <video poster="" id="street" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/sidewalk_canvas.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-real-office">
                        <video poster="" id="real-office" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/real_office_canvas.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>    
                            Real-life robot navigation involves more than just reaching a destination; 
                            it requires optimizing movements while addressing scenario-specific goals. 
                            An intuitive way for humans to express these goals is through abstract cues 
                            like verbal commands or rough sketches. Such human guidance may lack details 
                            or be noisy. Nonetheless, we expect robots to navigate as intended. 
                            For robots to interpret and execute these abstract instructions in line with 
                            human expectations, they must share a common understanding of basic navigation 
                            concepts with humans. 
                        </p>
                        <p>
                            To this end, we introduce CANVAS, a novel framework that combines visual and 
                            linguistic instructions for commonsense-aware navigation. Its success is driven 
                            by imitation learning, enabling the robot to learn from human navigation behavior. 
                            We present COMMAND, a comprehensive dataset with human-annotated navigation results, 
                            spanning over <strong>48 hours</strong> and <strong>219 km</strong>, designed to train commonsense-aware navigation systems 
                            in simulated environments. Our experiments show that CANVAS outperforms the strong 
                            rule-based system ROS NavStack across all environments, demonstrating superior performance 
                            with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 
                            <strong><u>0%</u></strong> total success rate, CANVAS achieves a total success rate of <strong><u>67%</u></strong>. CANVAS also closely 
                            aligns with human demonstrations and commonsense constraints, even in unseen environments. 
                            Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a 
                            total success rate of <strong><u>69%</u></strong>, highlighting the potential of learning from human demonstrations 
                            in simulated environments for real-world applications
                        </p>
                    </div>
                </div>
            </div>

            <!--/ Abstract. -->

            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Summary Video</h2>
                    <div class="publication-video">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/oJuU4x02azI?si=YRwnQb3IzmKfxbTo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">COMMAND dataset</h2>
                    <div class="content has-text-justified">
                        <figure id="data_pipeline">
                            <img src="./static/images/2_data_pipeline.png" alt="data_pipeline" />
                        </figure>
                        <p>
                            The COMMAND dataset is a comprehensive dataset that includes human-annotated navigation results spanning over 48 hours and 219 kilometers, 
                            specifically designed to train commonsense-aware navigation systems in simulated environments.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">CANVAS Architecture</h2>
                    <div class="content has-text-justified">
                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                            <tr>
                                <td width="50%">
                                    <figure id="model">
                                        <img src="./static/images/3_framework.png" alt="model" />
                                    </figure>
                                </td>
                                <td width="50%">
                                    <p>
                                        CANVAS is a vision language action(VLA) model that utilizes a vision language model with a waypoint tokenizer to generate navigation waypoints 
                                    </p>
                                    <p>
                                        The front view image Xf (t) and canvas map image Xc(t) are processed through a vision encoder gϕ(·). Both the visual tokens τv and language tokens τl are then fed into the large language model denoted as fϕ(·), which outputs the waypoint tokens [w0, w1, w2, w3] = fϕ(τv, τl)
                                    </p>
                                    <p>
                                        By treating navigation as a classification task, CANVAS can manage multimodal distributions, enhancing both stability and accuracy in complex environments.
                                    </p>
                                </td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Rollout Example</h2>
                    <div class="content has-text-justified">
                        <figure id="main_exp">
                            <img src="./static/images/4_success_case_study.png" alt="main_exp" />
                        </figure>
                        <p>
                            The left side of the figure compares CANVAS-L and CANVAS-S, showing CANVAS-L using the crosswalk despite a misleading
                            sketch instruction. The right side compares CANVAS-L and NavStack, illustrating CANVAS-L avoiding small obstacles, such as rocks.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Experiments (Simulator)</h2>
                    <div class="content has-text-justified">
                        <figure id="main_exp">
                            <img src="./static/images/5_metric_table_sim.png" alt="main_exp" />
                        </figure>
                        <p>
                            In all of environments, including unseen environments (gallery), CANVAS demonstrates a success rate of over <b>67%</b>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Experiments (Real)</h2>
                    <div class="content has-text-justified">
                        <figure id="main_exp">
                            <img src="./static/images/6_metric_table_real.png" alt="main_exp" />
                        </figure>
                        <p>
                            In real, even though it was trained <strong>only on simulator data</strong>, CANVAS also demonstrates a success rate of <b>69%</b>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{choi2024canvas,
    title={CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction},
    author={Choi, Suhwan and Cho, Yongjun and Kim, Minchan and Jung, Jaeyoon and Joe, Myunchul and Park, Yubeen and Kim, Minseo and Kim, Sungwoong and Lee, Sungjae and Park, Hwiseong and others},
    journal={arXiv preprint arXiv:2410.01273},
    year={2024}
}</code></pre>
        </div>
    </section>
    <br>
    <center class="is-size-10">
        The website design was based on <a href="https://github.com/general-navigation-models/general-navigation-models.github.io"><span 
        class="dnerf">general-navigation-models</span></a> adapted from <a href="https://nerfies.github.io" class="external-link"><span
        class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
