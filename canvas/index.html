<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="We propose a framework for context-aware robot navigation that excels in both simulated and real-world environments.">
    <meta name="keywords"
        content="Vision-Language-Action (VLA) Models, Imitation Learning, Multimodal Instruction Following">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5J9LZW868J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-5J9LZW868J');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="../static/css/bulma.min.css">
    <link rel="stylesheet" href="../static/css/slick.css">
    <link rel="stylesheet" href="../static/css/slick-theme.css">
    <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="../static/css/index.css">
    <link rel="icon" href="./static/images/favicon.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="../static/js/fontawesome.all.min.js"></script>
    <script src="../static/js/slick.min.js"></script>
    <script src="../static/js/index.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="../">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="../canvas/">
                            CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction
                        </a>
                        <a class="navbar-item" href="../d2e/">
                            D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI
                        </a>
                    </div>
                </div>
            </div>
        </div>

        </div>
    </nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">CANVAS: Commonsense-Aware Navigation System
                            for Intuitive Human-Robot Interaction</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><a href="https://suhwanchoi.me/">Suhwan
                                    Choi</a><sup>†1</sup>,</span>
                            <span class="author-block"><a
                                    href="https://scholar.google.com/citations?user=VxekekYAAAAJ">Yongjun
                                    Cho</a><sup>†1</sup>,</span>
                            <span class="author-block"><a href="https://minchankim.me/">Minchan
                                    Kim</a><sup>†1</sup>,</span>
                            <span class="author-block"><a
                                    href="https://scholar.google.com/citations?user=aQu1eZYAAAAJ">Jaeyoon
                                    Jung</a><sup>†1</sup>,</span>
                            <span class="author-block"><a
                                    href="https://scholar.google.com/citations?user=eDX87u8AAAAJ">Myunchul
                                    Joe</a><sup>1</sup>,</span>
                            <span class="author-block"><a
                                    href="https://www.linkedin.com/in/yu-been-park-7223a6234/">Yubeen
                                    Park</a><sup>1</sup>,</span>
                            <br>
                            <span class="author-block"><a
                                    href="https://scholar.google.com/citations?user=GqQ7qTEAAAAJ">Minseo
                                    Kim</a><sup>2</sup>,</span>
                            <span class="author-block">Sungwoong Kim<sup>2</sup>,</span>
                            <span class="author-block">Sungjae Lee<sup>2</sup>,</span>
                            <span class="author-block">Hwiseong Park<sup>1</sup>,</span>
                            <span class="author-block"><a href="https://jiwanchung.github.io/">Jiwan
                                    Chung</a><sup>2</sup>,</span>
                            <span class="author-block"><a href="https://yj-yu.github.io/home/">Youngjae
                                    Yu</a><sup>2</sup>,</span>
                        </div>

                        <div class="is-size-6 publication-authors">
                            <span class="author-block">
                                <sup>†</sup> Equal contribution, <sup>1</sup> MAUM.AI, <sup>2</sup> Yonsei University
                            </span>
                        </div>
                        <br>

                        <div class="is-size-5 publication-venue">
                            <span class="venue-block">International Conference on Robotics and Automation (ICRA)
                                2025</span>
                            <br>
                        </div>
                        <br>

                        <div class="is-size-6 publication-venue">
                            <span class="venue-block">
                                <span class="publication-awards">Outstanding Paper Award</span>
                                at NeurIPS 2024 Workshop Open-World Agents
                            </span>
                            <br>
                            <span class="venue-block">
                                <span class="publication-awards">Oral Talk</span>
                                at NeurIPS 2024 Workshop Open-World Agents
                            </span>
                            <br>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2410.01273"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <!-- Video Link -->
                                <span class="link-block">
                                    <a href="https://youtu.be/oJuU4x02azI"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>
                                <!-- Model Link -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/maum-ai/CANVAS-S"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-robot"></i>
                                        </span>
                                        <span>Model</span>
                                    </a>
                                </span>
                                <!-- Dataset Link -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/datasets/maum-ai/COMMAND"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i>
                                        </span>
                                        <span>Data</span>
                                    </a>
                                </span>
                                <!-- Demo Link -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/spaces/maum-ai/CANVAS-DEMO"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-images"></i>
                                        </span>
                                        <span>Demo</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop is-centered has-text-justified is-size-5">
            <div class="hero-body">
                <figure id="teaser">
                    <img src="./static/images/1_teaser.png" alt="canvas teaser" />
                </figure>
                <p>
                    CANVAS is a novel framework for commonsense-aware robot navigation that excels in both simulated and
                    real-world environments.
                </p>
            </div>
        </div>
    </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-orchard">
                        <video poster="" id="orchard" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/orchard_canvas.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-street">
                        <video poster="" id="street" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/sidewalk_canvas.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-real-office">
                        <video poster="" id="real-office" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/real_office_canvas.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Real-life robot navigation involves more than just reaching a destination;
                            it requires optimizing movements while addressing scenario-specific goals.
                            An intuitive way for humans to express these goals is through abstract cues
                            like verbal commands or rough sketches. Such human guidance may lack details
                            or be noisy. Nonetheless, we expect robots to navigate as intended.
                            For robots to interpret and execute these abstract instructions in line with
                            human expectations, they must share a common understanding of basic navigation
                            concepts with humans.
                        </p>
                        <p>
                            To this end, we introduce CANVAS, a novel framework that combines visual and
                            linguistic instructions for commonsense-aware navigation. Its success is driven
                            by imitation learning, enabling the robot to learn from human navigation behavior.
                            We present COMMAND, a comprehensive dataset with human-annotated navigation results,
                            spanning over <strong>48 hours</strong> and <strong>219 km</strong>, designed to train
                            commonsense-aware navigation systems
                            in simulated environments. Our experiments show that CANVAS outperforms the strong
                            rule-based system ROS NavStack across all environments, demonstrating superior performance
                            with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a
                            <strong><u>0%</u></strong> total success rate, CANVAS achieves a total success rate of
                            <strong><u>67%</u></strong>. CANVAS also closely
                            aligns with human demonstrations and commonsense constraints, even in unseen environments.
                            Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a
                            total success rate of <strong><u>69%</u></strong>, highlighting the potential of learning
                            from human demonstrations
                            in simulated environments for real-world applications
                        </p>
                    </div>
                </div>
            </div>

            <!--/ Abstract. -->

            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Summary Video</h2>
                    <div class="publication-video">
                        <iframe width="560" height="315"
                            src="https://www.youtube.com/embed/oJuU4x02azI?si=YRwnQb3IzmKfxbTo"
                            title="YouTube video player" frameborder="0"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">COMMAND dataset</h2>
                    <div class="content has-text-justified">
                        <figure id="data_pipeline">
                            <img src="./static/images/2_data_pipeline.png" alt="data_pipeline" />
                        </figure>
                        <p>
                            The COMMAND dataset is a comprehensive dataset that includes human-annotated navigation
                            results spanning over 48 hours and 219 kilometers,
                            specifically designed to train commonsense-aware navigation systems in simulated
                            environments.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">CANVAS Architecture</h2>
                    <div class="content has-text-justified">
                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                            <tr>
                                <td width="50%">
                                    <figure id="model">
                                        <img src="./static/images/3_framework.png" alt="model" />
                                    </figure>
                                </td>
                                <td width="50%">
                                    <p>
                                        CANVAS is a vision language action(VLA) model that utilizes a vision language
                                        model with a waypoint tokenizer to generate navigation waypoints
                                    </p>
                                    <p>
                                        The front view image Xf (t) and canvas map image Xc(t) are processed through a
                                        vision encoder gϕ(·). Both the visual tokens τv and language tokens τl are then
                                        fed into the large language model denoted as fϕ(·), which outputs the waypoint
                                        tokens [w0, w1, w2, w3] = fϕ(τv, τl)
                                    </p>
                                    <p>
                                        By treating navigation as a classification task, CANVAS can manage multimodal
                                        distributions, enhancing both stability and accuracy in complex environments.
                                    </p>
                                </td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Rollout Example</h2>
                    <div class="content has-text-justified">
                        <figure id="main_exp">
                            <img src="./static/images/4_success_case_study.png" alt="main_exp" />
                        </figure>
                        <p>
                            The left side of the figure compares CANVAS-L and CANVAS-S, showing CANVAS-L using the
                            crosswalk despite a misleading
                            sketch instruction. The right side compares CANVAS-L and NavStack, illustrating CANVAS-L
                            avoiding small obstacles, such as rocks.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Experiments (Simulator)</h2>
                    <div class="content has-text-justified">
                        <figure id="main_exp">
                            <img src="./static/images/5_metric_table_sim.png" alt="main_exp" />
                        </figure>
                        <p>
                            In all of environments, including unseen environments (gallery), CANVAS demonstrates a
                            success rate of over <b>67%</b>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Experiments (Real)</h2>
                    <div class="content has-text-justified">
                        <figure id="main_exp">
                            <img src="./static/images/6_metric_table_real.png" alt="main_exp" />
                        </figure>
                        <p>
                            In real, even though it was trained <strong>only on simulator data</strong>, CANVAS also
                            demonstrates a success rate of <b>69%</b>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@INPROCEEDINGS{choi2025canvas,
  author={Choi, Suhwan and Cho, Yongjun and Kim, Minchan and Jung, Jaeyoon and Joe, Myunchul and Park, Yubeen and Kim, Minseo and Kim, Sungwoong and Lee, Sungjae and Park, Hwiseong and Chung, Jiwan and Yu, Youngjae},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)},
  title={CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction},
  year={2025},
  volume={},
  number={},
  pages={2445-2452},
  keywords={Visualization;Navigation;Imitation learning;Human-robot interaction;Linguistics;Trajectory;Noise measurement;Collision avoidance;Robots;Commonsense reasoning},
  doi={10.1109/ICRA55743.2025.11127664}}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container is-max-desktop content has-text-centered is-size-7">
            <p>
                The website design was based on
                <a href="https://github.com/general-navigation-models/general-navigation-models.github.io">
                    <span class="dnerf">general-navigation-models</span>
                </a>
                adapted from
                <a href="https://nerfies.github.io" class="external-link">
                    <span class="dnerf">Nerfies</span>
                </a>.
            </p>
        </div>
    </footer>

</body>

</html>